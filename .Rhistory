# 0: IMC < 25: No sobrepeso
datos[["EN"]] <- ifelse(datos[["IMC"]] >= 25, 1, 0)
# Se factoriza esta columna
datos[["EN"]] <- factor(datos[["EN"]])
# Se verifica que se hayan agregado correctamente las columnas
head(datos)
# Enunciado
# 1. Definir la semilla a utilizar, que corresponde a los primeros cinco dígitos
# del RUN del integrante de mayor edad del equipo.
set.seed(20785)
# 2. Seleccionar una muestra de 100 personas, asegurando que la mitad tenga
# estado nutricional "sobrepeso" y la otra mitad "no sobrepeso".
sobrepeso <- datos %>% filter(EN == 1) %>% sample_n(50)
nosobrepeso <- datos %>% filter(EN == 0) %>% sample_n(50)
# Se unen las dos muestras
muestra <- rbind(sobrepeso, nosobrepeso)
# Se eliminan las variables IMC y EN del conjunto
datos_n <- muestra %>% select(-c(IMC, EN))
# Se hace el modelo con un máximo de 8 predictores
modelo <- regsubsets(Weight ~ ., data = datos_n, nbest = 1, nvmax = 8,
method = "exhaustive")
# Se muestra el gráfico
plot(modelo)
# Se seleccionan solo estos predictores (columnas)
datos_n <- datos_n %>% select(Weight, Waist.Girth, Forearm.Girth, Height)
# Se ajusta el modelo usando boostrapping
modelo_n <- train(Weight ~ ., data = datos_n, method = "lm",
trControl = trainControl(method = "boot", number = 999))
# Se imprimen los resultados
summary(modelo_n)
library(tidyverse)
library(ggpubr)
library(dplyr)
library(boot)
library(simpleboot)
library(car)
library(caret)
datos = read.csv2("EI-2023-2-PE2-Datos-Equipo08.csv", sep = ",")
# Distribución cercana a la normal, que se compprueba con la prueba de Shapiro Wilk:
shapiro.test(datos$X2)
shapiro.test(datos$X6)
# Mediante el test de Shapiro Wilk, se puede confirmar que las distribuciones
# no se asemejan a la distribución normal, esto debido a que ambos p-value son
# menores que 0.05. Debido a esto es necesario realizar un remuestreo para
# lograr trabajar con los datos problemáticos.
# En esta oportunidad, se realizará un remuestreo con Bootstraping con 300
# iteraciones. Para ello, primero se debe definir la cantidad de iteraciones.
iteraciones <- 300;
# Se seleccionan los promedios de X2 y X6
datosPregunta2 <- datos %>% select(X2, X6);
#
remuestreo <- train(X2 ~ ., data = datosPregunta2, method = "lm",
trControl = trainControl(method = "boot",
number = iteraciones))
# Y se recopilan los resultados
summary(remuestreo)
View(datosPregunta2)
# Distribución cercana a la normal, que se compprueba con la prueba de Shapiro Wilk:
shapiro.test(datos$X2)
shapiro.test(datos$X6)
#
remuestreo <- train(X2 ~ ., data = datosPregunta2, method = "lm",
trControl = trainControl(method = "boot",
number = iteraciones))
# Y se recopilan los resultados
summary(remuestreo)
source("~/Equipo08.R", echo=TRUE)
update.packages(ask = FALSE, checkBuilt = TRUE)
source("~/Repositorio-POR/anovaMI.R", echo=TRUE)
# Lectura de datos
datos = read.csv2("/home/dilget/Repositorio-POR/", sep = ",");
# Lectura de datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP05 Datos.csv", sep = ",");
View(datos)
help("filter")
# Se filtran los datos para tener solo los requeridos
muestraMedia <- datos %>% filter(dificultad = "media");
# Se filtran los datos para tener solo los requeridos
muestraMedia <- datos %>% filter(dificultad == "media");
View(muestraMedia)
# Se filtran los datos para tener solo los requeridos
muestraMedia <- datos %>% filter(dificultad == "Media");
muestraMedia <- muestraMedia %>% filter(area == "Leyes" |
area == "Música" |
area == "Matemáticas");
knitr::opts_chunk$set(echo = TRUE)
datos <- read.csv2("EP05 Datos.csv", sep = ",")
muestra <- datos %>% filter(dificultad == "Media")
muestra <- muestra %>% filter(area == "Leyes" |
area == "Música" |
area == "Matemáticas")
muestra_ancha <- muestra %>% pivot_wider(names_from = "area",
values_from = "tiempo")
#Datos ancho
muestra_ancha
#Datos Largos
head(muestra)
# Lectura de datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP05 Datos.csv", sep = ",");
# Se filtran los datos para tener solo los requeridos
muestraMedia <- datos %>% filter(dificultad == "Media");
muestraMedia <- muestraMedia %>% filter(area == "Leyes" |
area == "Música" |
area == "Matemáticas");
# Se ponen los datos a lo ancho
muestra_ancha <- muestraMedia %>% pivot_wider(names_from = "area",
values_from = "tiempo");
# Se separan muestras para cada una de las distintas variables buscadas
leyes <- muestraMedia %>% filter(area == "Leyes");
musica <- muestraMedia %>% filter(area == "Música");
matematicas <- muestraMedia %>% filter(area == "Matemáticas");
# Se comprueba la normalidad de las variables
shapiro.test(leyes$tiempo)
shapiro.test(musica$tiempo)
shapiro.test(matematicas$tiempo)
help("ezANOVA")
# Lectura de datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP05 Datos.csv", sep = ",");
# Se filtran los datos para tener solo los requeridos
muestraMedia <- datos %>% filter(dificultad == "Media");
muestraMedia <- muestraMedia %>% filter(area == "Leyes" |
area == "Música" |
area == "Matemáticas");
# Se ponen los datos a lo ancho
muestra_ancha <- muestraMedia %>% pivot_wider(names_from = "area",
values_from = "tiempo");
# Se separan muestras para cada una de las distintas variables buscadas
leyes <- muestraMedia %>% filter(area == "Leyes");
musica <- muestraMedia %>% filter(area == "Música");
matematicas <- muestraMedia %>% filter(area == "Matemáticas");
# Se comprueba la normalidad de las variables
shapiro.test(leyes$tiempo)
shapiro.test(musica$tiempo)
shapiro.test(matematicas$tiempo)
# Se realiza ANOVA
resultado = ezANOVA(muestra, dv = tiempo, wid = id, between = area, return_aov = TRUE)
# Se comprueba la normalidad de las variables
shapiro.test(leyes$tiempo)
shapiro.test(musica$tiempo)
shapiro.test(matematicas$tiempo)
# Se realiza ANOVA
resultado = ezANOVA(muestraMedia, dv = tiempo, wid = id, between = area, return_aov = TRUE)
resultado$`Levene's Test for Homogeneity of Variance`
summary(resultado$aov)
knitr::opts_chunk$set(echo = TRUE)
# Librerías
library(dplyr)
library(tidyr)
library(ggpubr)
library(ez)
datos <- read.csv2("EP05 Datos.csv", sep = ",")
muestra <- datos %>% filter(dificultad == "Media")
muestra <- muestra %>% filter(area == "Leyes" |
area == "Música" |
area == "Matemáticas")
muestra_ancha <- muestra %>% pivot_wider(names_from = "area",
values_from = "tiempo")
#Datos ancho
muestra_ancha
#Datos Largos
head(muestra)
leyes <- muestra %>% filter(area == "Leyes")
musica <- muestra %>% filter(area == "Música")
mates <- muestra %>% filter(area == "Matemáticas")
muestra2 <-  rbind(leyes, musica, mates)
muestra2[["area"]] <- factor(muestra2[["area"]])
g <- ggqqplot(muestra2,
x = "tiempo",
y = "area",
color = "area")
g <- g + facet_wrap(~ area)
g <- g + rremove("x.ticks") + rremove("x.text")
g <- g + rremove("y.ticks") + rremove("y.text")
g <- g + rremove("axis.title")
g
# Normalidad Leyes
testle <- shapiro.test(leyes$tiempo)
testle
# Normalidad Música
testmu <- shapiro.test(musica$tiempo)
testmu
# Normalidad Matemáticas
testma <- shapiro.test(mates$tiempo)
testma
muestra[["area"]] <- factor(muestra[["area"]])
muestra[["id"]] <- factor(muestra[["id"]])
prueba <- ezANOVA(muestra,
dv = tiempo,
wid = id,
between = area,
return_aov = TRUE)
prueba$`Levene's Test for Homogeneity of Variance`
summary(prueba$aov)
source("~/Repositorio-POR/anovaMI.R", echo=TRUE)
help("factor")
source("~/Repositorio-POR/anovaMI.R", echo=TRUE)
source("~/Repositorio-POR/anovaMI.R", echo=TRUE)
source("~/Repositorio-POR/anovaMI.R", echo=TRUE)
source("~/Repositorio-POR/anovaMI.R", echo=TRUE)
# Librerias
library(ez)
library(tidyr)
library(dplyr)
library(ggpubr)
# Se leen los datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP05 Datos.csv", sep = ",");
View(datos)
# Se filtran los datos requeridos
muestra = datos %>% filter(area == "Biología")
baja <- muestra %>% filter(dificultad == "Baja");
media <- muestra %>% filter(dificultad == "Media");
alta <- muestra %>% filter(dificultad == "Alta");
View(media)
View(baja)
View(alta)
# Condiciones
shapiro.test(baja)
# Condiciones
shapiro.test(baja$tiempo)
shapiro.test(media$tiempo)
shapiro.test(alta$tiempo)
# Librerías
library(ez)
library(tidyr)
library(dplyr)
library(ggpubr)
# Se leen los datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP05 Datos.csv", sep = ",");
# Se filtran los datos requeridos
muestra = datos %>% filter(area == "Biología");
muestra <- muestra %>% filter(dificultad == "Baja" |
dificultad == "Media" |
dificultad == "Alta");
# Librerías
library(ez)
library(tidyr)
library(dplyr)
library(ggpubr)
# Se leen los datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP05 Datos.csv", sep = ",");
# Se filtran los datos requeridos
muestra = datos %>% filter(area == "Biología");
baja <- muestra %>% filter(dificultad == "Baja");
media <- muestra %>% filter(dificultad == "Media");
alta <- muestra %>% filter(dificultad == "Alta");
# Condiciones:
# Normalidad
shapiro.test(baja$tiempo)
shapiro.test(media$tiempo)
shapiro.test(alta$tiempo)
# Factores
# TODO: Entender el por que
muestra[["dificultad"]] <- factor(muestra[["dificultad"]])
muestra[["id"]] <- factor(muestra[["id"]])
prueba <- ezANOVA(data = muestra,
dv = tiempo,
wid = id,
within = area,
return_aov = TRUE)
# Condiciones:
# Normalidad
shapiro.test(baja$tiempo)
shapiro.test(media$tiempo)
shapiro.test(alta$tiempo)
# Factores
# TODO: Entender el por que
muestra[["dificultad"]] <- factor(muestra[["dificultad"]])
muestra[["id"]] <- factor(muestra[["id"]])
prueba <- ezANOVA(data = muestra,
dv = tiempo,
wid = id,
within = dificultad,
return_aov = TRUE)
summary(prueba$aov)
knitr::opts_chunk$set(echo = TRUE)
libraries <- c("ggpubr", "PASWR2", "tidyverse","dplyr", "ez", "nlme", "emmeans")
lapply(libraries, function(package) {
if (!require(package, character.only = TRUE)) {
install.packages(package, dependencies = TRUE)
library(package, character.only = TRUE)
} else {
library(package, character.only = TRUE)
}
})
datos <- read.csv2("EP05 Datos.csv", header = TRUE, sep = ",")
head(datos)
datos3Grupos <- datos %>% filter(area == "Biología")
instancia <- factor(1:200)
datosAlta <- datos3Grupos %>% filter(dificultad == "Alta")
datosMedia <- datos3Grupos %>% filter(dificultad == "Media")
datosBaja <- datos3Grupos %>% filter(dificultad == "Baja")
datos <- data.frame(instancia, datosAlta$tiempo, datosMedia$tiempo, datosBaja$tiempo)
datos <- datos %>% pivot_longer(c("datosAlta.tiempo", "datosMedia.tiempo", "datosBaja.tiempo"), names_to = "dificultad", values_to = "tiempo")
datos[["dificultad"]] <- factor(datos[["dificultad"]])
g <- ggqqplot(datos, x = "tiempo", y = "dificultad", color = "dificultad")
g <- g + facet_wrap(~ dificultad)
g <- g + rremove("x.ticks") + rremove("x.text")
g <- g + rremove("y.ticks") + rremove("y.text")
g <- g + rremove("axis.title")
print(g)
prueba <- ezANOVA(data = datos, dv = tiempo, within = dificultad,
wid = instancia, return_aov = TRUE)
summary(prueba$aov)
# Factores
# TODO: Entender el por que
muestra[["dificultad"]] <- factor(muestra[["dificultad"]])
muestra[["id"]] <- factor(muestra[["id"]])
prueba <- ezANOVA(data = muestra,
dv = tiempo,
wid = id,
within = dificultad,
return_aov = TRUE)
summary(prueba$aov)
prueba[["Mauchly's Test for Sphericity"]]
prueba <- ezANOVA(data = muestra,
dv = tiempo,
wid = id,
within = dificultad,
return_aov = TRUE)
summary(prueba$aov)
exit
clear
cl
# Librerías
library(boot)
library(tidyverse)
library(ez)
library(ggpubr)
library(multcomp)
# Lectura de datos
datos = read.csv2("EP08 Datos CASEN 2017.csv", sep = ";", fileEncoding = "latin1")
# Lectura de datos
datos = read.csv2("/home/dilget/Repositorio-POR/EP08 Datos CASEN 2017.csv", sep = ";", fileEncoding = "latin1")
View(datos)
muestra <- datos %>% filter(region == "Región Metropolitana de Santiago")
View(muestra)
View(muestra)
muestra <- muestra %>% filter(ecivil == "Casado(a)")
View(muestra)
muestra <- datos %>% filter(region == "Región Metropolitana de Santiago")
muestra <- muestra %>% filter(ecivil == "Casado(a)")
muestra <- muestra %>% filter(o12 == "Permanente")
muestra <- datos %>% filter(region == "Región Metropolitana de Santiago")
muestra <- muestra %>% filter(ecivil == "Casado(a)")
muestra <- muestra %>% filter(o12 == "Permanente?")
# Lectura de datos
datos <- read.csv(file = "/home/dilget/Repositorio-POR/EP09 Datos.csv", stringsAsFactors = TRUE)
# Lectura de datos
datos <- read.csv(file = "~/Repositorio-POR/EP09 Datos.csv", stringsAsFactors = TRUE)
# Librerías
library(dplyr)
library(ggpubr)
library(caret)
library(lmtest)
library(car)
# Lectura de datos
datos <- read.csv(file = "~/Repositorio-POR/EP09 Datos.csv", stringsAsFactors = TRUE)
# Lectura de datos
datos <- read.csv2(file = "~/Repositorio-POR/EP09 Datos.csv")
View(datos)
# Definimos una semilla
set.seed(1624)
#
datosMujeres <- datos %>% filter(Gender == 1)
datos <- sample_n(datos, 50, replace = FALSE)
# Lectura de datos
datos <- read.csv2(file = "~/Repositorio-POR/EP09 Datos.csv")
# Definimos una semilla
set.seed(1624)
#
datosMujeres <- datos %>% filter(Gender == 1)
muestraMujeres <- sample_n(datos, 50, replace = FALSE)
View(muestraMujeres)
#
datosMujeres <- datos %>% filter(Gender == 1)
muestraMujeres <- sample_n(datosMujeres, 50, replace = FALSE)
#
datosMujeres <- datos %>% filter(Gender == 1)
muestra <- datosMujeres[sample(nrow(datosMujeres),50), ]
rm(muestraMujeres)
# Lectura de datos
datos <- read.csv2(file = "~/Repositorio-POR/EP09 Datos.csv")
# Definimos una semilla
set.seed(1624)
#
datosMujeres <- datos %>% filter(Gender == 1)
muestra <- datosMujeres[sample(nrow(datosMujeres),50), ]
View(muestra)
# 3. Seleccionar aleatoriamente 8 posibles variables predictoras
predictoras <- sample(24,8)
predictoras
# 4. Elegir otra variable que no este en la seleccionada
# La variable de interés es el peso (weight)
correlaciones <- cor(muestra)
View(correlaciones)
max_cor <- max(correlation[23,-23])
# 4. Elegir otra variable que no este en la seleccionada
# La variable de interés es el peso (weight)
correlaciones <- cor(muestra)
max_cor <- max(correlaciones[23,-23])
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggpubr)
library(caret)
library(lmtest)
library(car)
mujeres <- filter(datos, Gender == 0)
mujeres <- filter(datos, Gender == 0)
muestra <- mujeres[sample(nrow(mujeres), 50), ]  #Se selecciona una miestra de 50 mujeres
**3. Seleccionar de forma aleatoria ocho posibles variables predictoras.**
```{r}
predictores <- sample(24, 8) #Se selecciona de forma aleatoria ocho posibles variables predictoras.
predictores
predictores <- sample(24, 8) #Se selecciona de forma aleatoria ocho posibles variables predictoras.
predictores
**4. Seleccionar, de las otras variables, una que el equipo considere que podría ser útil para predecir la variable Peso, justificando bien esta selección.**
Se realiza un análisis de correlación con respecto a la variable de interés, que en este caso es "Weight" (Peso).El análisis de correlación es una etapa clave en la identificación de variables potencialmente relevantes para el modelo, ya que permite evaluar la relación inicial entre las variables y la variable objetivo, facilitando la toma de decisiones en la selección de predictores para la construcción del modelo de regresión.
```{r}
correlacion <- cor(muestra)
max_cor <- max(correlacion[23, -23])
indice_max_cor <- which.max(correlacion[23, -23])
indice_max_cor
# 4. Elegir otra variable que no este en la seleccionada
# La variable de interés es el peso (weight)
correlaciones <- cor(muestra)
max_cor <- max(correlaciones[23,-23])
indice_max_cor <- which.max(correlacion[23, -23])
indice_max_cor
# 5. Realizar una regresión lineal simple con la variable del paso 4.
muestraP <- muestra[predictoras]
muestraP$Weight <- muestra$Weight
muestraP$Hip.Girth <- muestra$Hip.Girth
View(muestra)
View(muestraP)
head(muestraP)
modelo <- lm(Weight ~ Hip.Girth, data = muestra)
print(summary(modelo))
indice_max_cor <- which.max(correlacion[23, -23])
indice_max_cor
# 5. Realizar una regresión lineal simple con la variable del paso 4.
muestraP <- muestra[predictoras]
muestraP$Weight <- muestra$Weight
muestraP$Hip.Girth <- muestra$Hip.Girth
modelo <- lm(Weight ~ Hip.Girth, data = muestra)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggpubr)
library(caret)
library(lmtest)
library(car)
set.seed(1624) #Se Define la semilla
datos <- read.csv2("EP09 Datos.csv")
head(datos)
mujeres <- filter(datos, Gender == 0)
muestra <- mujeres[sample(nrow(mujeres), 50), ]  #Se selecciona una miestra de 50 mujeres
predictores <- sample(24, 8) #Se selecciona de forma aleatoria ocho posibles variables predictoras.
predictores
correlacion <- cor(muestra)
max_cor <- max(correlacion[23, -23])
indice_max_cor <- which.max(correlacion[23, -23])
indice_max_cor
library(dplyr)
library(ggpubr)
library(caret)
library(lmtest)
library(car)
set.seed(1624) #Se Define la semilla
datos <- read.csv2("EP09 Datos.csv", stringsAsFactors = TRUE)
setwd("/home/dilget/Repositorio-POR/")
set.seed(1624) #Se Define la semilla
datos <- read.csv2("EP09 Datos.csv", stringsAsFactors = TRUE)
mujeres <- filter(datos, Gender == 0)
mujeres <- mujeres[sample(nrow(mujeres),50), ]
columnas_restantes <- mujeres %>%select(-c(Shoulder.Girth,
Bitrochanteric.diameter,
Ankle.Minimum.Girth,
Ankles.diameter,
Wrists.diameter,
Knees.diameter,
Thigh.Girth,
Waist.Girth))
#Se busca la mejor relacion lineal.
correlaciones <- cor(columnas_restantes)
max_cor <- which.max(correlaciones[15, -15])
print(max_cor)
#Hip.Girth
columnas_seleccionadas <- mujeres %>% select(c(Shoulder.Girth,
Hip.Girth,
Bitrochanteric.diameter,
Ankle.Minimum.Girth,
Ankles.diameter,
Wrists.diameter,
Knees.diameter,
Thigh.Girth,
Waist.Girth,
Weight))
#Se crea el modelo con validacion cruzada
modelo <- train(Weight ~ Hip.Girth, data = columnas_seleccionadas, method = "lm",
trControl = trainControl(method = "cv", number = 10))
modelo <- modelo[["finalModel"]]
# Se procede a obtener los residuos y estadísticas de influencia
# de los casos
evaluacion_modelo <- data.frame(predicted.probabilities =
fitted(modelo))
evaluacion_modelo[["residuos_estandarizados"]] <- rstandard(modelo)
evaluacion_modelo[["residuos_estudiantizados"]] <- rstudent(modelo)
evaluacion_modelo[["distancia_cook"]] <- cooks.distance(modelo)
evaluacion_modelo[["dfbeta"]] <- dfbeta(modelo)
evaluacion_modelo[["dffit"]] <- dffits(modelo)
evaluacion_modelo[["apalancamiento"]] <- hatvalues(modelo)
evaluacion_modelo[["covratio"]] <- covratio(modelo)
cat("Influencia de los casos: \n")
